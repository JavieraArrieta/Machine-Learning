![HenryLogo](https://d31uz8lwfmyn8g.cloudfront.net/Assets/logo-henry-white-lg.png)

# **Aprendizaje no supervisado**

Lleg√≥ el momento de dejar atr√°s, al menos por un rato, al **aprendizaje supervisado**. Ahora nos abocaremos al estudio del **aprendizaje no supervisado**.

En esta etapa cambiaremos el enfoque con el que ven√≠amos trabajando hasta el momento. `No supervisado` significa que ya no contamos con las etiquetas asociadas a nuestros datos. Es decir que las instancias no tienen asignadas una clase o un valor de salida. 

Muchas veces se indica que esta subrama de Machine Learning se aplica para encontrar patrones.

Pues esto es as√≠. El objetivo aqu√≠ ya no consiste en predecir la etiqueta, sino -precisamente- en encontrar patrones en el set de datos.

Veremos dos t√©cnicas de aprendizaje no supervisado. A saber:

+ Clustering

+ Reducci√≥n de dimensionalidad

- - -

## ***Clustering***

T√©cnica para agrupar datos de acuerdo a cu√°nto se parecen entre s√≠. Dado un set de datos, nuestra meta ser√° encontrar grupos -clusters- en los cuales las instancias pertenecientes sean parecidas -est√©n cerca-.

<img src = "../_src/assets/clust.png" height = 350>

**Pero ¬øc√≥mo hacemos para agrupar los datos?**

<img src = "../_src/assets/hom.png" height = 200>

üö® Spoiler Alert: como no pod√≠a ser de otra forma, existen diversos algoritmos que nos facilitan esta tarea. Ellos son:

+ K-means

+ DBSCAN

+ Hierarchical clustering

+ Fuzzy C-means

+ GMM

El alcance de este curso cubrir√° los dos primeros.

### `K-means`

El fundamento de este algoritmo radica en separar los datos en K clusters, ubicando a las instancias que est√©n dentro de una regi√≥n cercana de un mismo cluster.

<img src = "../_src/assets/kmeans.png" height = 150>

El centro de cada cluster -centroide- es el promedio de todos los puntos pertenecientes a ese cluster.

Cada punto est√° m√°s cerca del centro de su cluster que de cualquier otro centroide.

Trabaja de manera iterativa:

1) Se inicializan los K centroides

2) Se asigna cada instancia al centroide m√°s cercano

3) Se actualizan los centroides -la nueva posici√≥n del centroide es el promedio de las posiciones de las instancias en ese cluster (means)-

4) Se repiten los pasos 2 y 3 -hasta que la posici√≥n del centroide no var√≠e- 

Recomendamos el siguiente [enlace](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) de la Stanford University sobre el funcionamiento de este algoritmo.

### `DBSCAN`

*Density-Based Spatial Clustering of Applications with Noise*.

<img src = "https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/53842/versions/4/screenshot.png" height = 250 >

Quiz√°s es un poco menos utilizado que el anterior, pero no por ello menos importante. No es necesario aqu√≠ seleccionar la cantidad de clusters, ya que el modelo los definir√° autom√°ticamente a partir de la densidad de puntos y no de centroides. Tambi√©n incorpora la dimensi√≥n de outliers -ruido-, los cuales deja sin clasificar.

El modelo permite agrupar formas m√°s complejas, no necesariamente globulares.

Los cluster son regiones densas en el espacio de datos. Cada punto del cluster tiene que tener un m√≠nimo de vecinos en un radio determinado para no ser un outlier.

Hay tres tipos de puntos:

+ Core point

+ Border point

+ Noise

<img src = "https://miro.medium.com/max/627/1*yT96veo7Zb5QeswV7Vr7YQ.png" height = 250>

Hiperpar√°metros claves del algoritmo:

+ Epsilon: magnitud del radio

+ MinPoints: cantidad m√≠nima de vecinos

Para profundizar en este algoritmo, este [link](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html) ser√° de gran ayuda.

Respecto a la elecci√≥n de uno u otro, depender√° del tipo de datos con el que contemos. Podemos se√±alar que `K-means` se adec√∫a mejor a clusters alejados, bien agrupados y globulares. Por su parte, DBSCAN es m√°s flexible y permite adaptarse a formas m√°s complejas.
- - -

## **M√©tricas de evaluaci√≥n**

Veremos, ahora, las m√©tricas existentes para evaluar los modelos de aprendizaje no supervisado. A diferencia del paradigma del aprendizaje supervisado, que estuvimos viendo hasta hoy, aqu√≠ no tenemos etiquetas para comparar cu√°n alejado de ellas estuvo nuestro valor predicho. 

Pasemos a ver qu√© opciones tenemos para evaluar nuestros modelos en el aprendizaje no supervisado.

### `Elbow`

Se emplea para el algoritmo K-means. Busca la mejor cantidad K de clusters. Para ello, mide la distancia de cada punto al centroide m√°s cercano. Luego de entrenar el modelo, la variable `model.inertia` tiene esta informaci√≥n.

<img src = "https://i.imgur.com/DMXslDR.png" height = 300>

K √≥ptimo => buscar d√≥nde est√° el codo de la curva. El valor de inercia siempre desciende con el n√∫mero de clusters.

Si utilizamos como K los valores que est√°n a la derecha del K √≥ptimo, nuestro modelo comenzar√° a overfittear.

### `Silhouette`

Con esta m√©trica medimos qu√© tan parecidos son los datos con su propio cluster -cohesi√≥n- en comparaci√≥n con qu√© tan parecidos son a otros clusters -separaci√≥n-.

<img src = "../_src/assets/silo.png" height = 80>

Se utiliza para cualquier t√©cnica de clustering.

Su valor oscila entre -1 y 1. Un valor alto indica que el objeto est√° bien emparejado con su propio cluster y mal emparejado con los clusters vecinos. Si la mayor√≠a de los objetos tienen un valor alto, entonces la configuraci√≥n del cluster es apropiada. Si muchos puntos tienen un valor bajo o negativo, entonces la configuraci√≥n de cluster puede tener demasiados o muy pocos clusters.

<img src = "https://bookdown.org/dparedesi/data-science-con-r/Data-Science-con-R_files/figure-html/unnamed-chunk-654-1.png" height = 300>

- - -

## ***Reducci√≥n de dimensionalidad***

La reducci√≥n de dimensionalidad busca disminuir la cantidad de features de un dataset, pero reteniendo la mayor cantidad de informaci√≥n posible.

Esto sirve para:

+ Mejorar eficiencia en modelos de regresi√≥n y clasificaci√≥n

+ Disminuir el ruido

+ Facilitar la visualizaci√≥n

+ Detectar features relevantes en datasets

Esta t√©cnica del aprendizaje no supervisado, generalmente, forma parte de la etapa de `preprocesamiento de datos`. Es decir, aplicamos primero este modelo previo a la utilizaci√≥n de otro modelo de Machine Learning.

Algoritmos utilizados para reducci√≥n de dimensionalidad:

+ SVD

+ PCA

+ MDS

+ t-SNE

+ Auto-encoders

+ LDA

Nuevamente, veremos los dos primeros algoritmos que son los m√°s utilizados en la materia.

### `SVD`

*Singular Value Decomposition*.

Es un m√©todo de √°lgebra lineal que nos permite representar cualquier matriz en t√©rminos de la multiplicaci√≥n de otras tres matrices.

<img src = "../_src/assets/svd.jpg" height = 300>

Entre sus tantas utilidades, podemos mencionar la de **reducir** una matriz M -pasar de tener muchos features a tener menos, pero que sean buenos-.

El objetivo consiste en reducir la cantidad de features. Para lograrlo, buscamos crear una nueva matriz B que reemplace a la M, para que tenga menos columnas -es decir, menos atributos-. Esto se conoce como **SVD truncado**.

<img src = "../_src/assets/svd_2.jpg" height = 300>

El hiperpar√°metro que debemos establecer en este modelo es **r**, que representa cu√°ntos features terminaremos teniendo.

### `PCA`

*Principal Component Analysis*.

El m√©todo de PCA nos permite reducir la dimensionalidad de un conjunto de datos, sin perder informaci√≥n esencial. Funciona transformando un conjunto de $d$ variables correlacionadas $X=[X_1,X_2,...,X_d]$ en un conjunto de variables no correlacionadas $W=[W_1,W_2,...,W_d]$.

A trav√©s de combinaciones lineales de las variables originales que maximizan la varianza explicada se consiguen los llamados componentes principales. Estos son ortogonales entre si (producto escalar 0).

PCA se basa en el teorema de descomposici√≥n espectral de la llamada matriz de varianzas y covarianzas $\Sigma_{d\times d}$

La primera componente principal est√° en la direcci√≥n donde los datos presentan varianza m√°xima. La segunda componente principal est√° en la segunda direcci√≥n en t√©rminos de varianza, y as√≠ sucesivamente.

Estas componentes pueden representar dimensiones no medibles/no medidas en nuestros datos pero que est√°n altamente correlacionadas: si poseemos, por ejemplo, una curva de bonos, teniendo **d** maturities (duraci√≥n del bono) diferentes, podemos conseguir **d** componentes principales. Una vez que conseguimos, observando el comportamiento de los primeros tres, podemos explicar cerca del 100% de la variabilidad de los datos y podemos dar interpretaciones a cada componente.

El hiperpar√°metro principal a establecer es la cantidad de variables con las que nos queremos quedar.

<<<<<<< HEAD
<img src = "../_src/assets/PCA.png" height = 300>

<img src = "../_src/assets/PCA2.png" height = 250>
=======
<img src = "../_src/assets/PCA.png" height = 250>

<img src = "../_src/assets/PCA2.png" height = 300>
>>>>>>> d48132453714123eeeb2410cacea843e6177b210

Dejamos un breve [video](https://www.youtube.com/watch?v=HMOI_lkzW08&ab_channel=StatQuestwithJoshStarmer) explicativo de el algoritmo PCA.

- - -

La pr√°ctica de la clase de hoy se conforma de la siguiente manera:

+ Pr√°ctica_01: modelos de clustering. Usaremos K-means y DBSCAN

+ Pr√°ctica_02: m√©tricas de evaluaci√≥n de clustering

+ Pr√°ctica_03: modelos de reducci√≥n de dimensionalidad. Usaremos SVD y PCA

- - -

**La clase que viene volveremos al aprendizaje supervisado.**

<img src = "https://www.pintzap.com/storage/img/memegenerator/templates/homer-simpson-yuju.jpg" height = 200>
